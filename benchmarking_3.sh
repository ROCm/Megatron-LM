#!/bin/bash

# NO_TORCH_COMPILE=1: do not use torch.compile (default=0)
# USE_FLASH_ATTN=1: use flash attention (default=0)
# TE_FP16=1: use transformer engine (only works for Nvidia GPUs) (default=0)
# NO_TRAINING=1: compute metrics from log file without running the training steps (default=0)
# SEQ_PARALLEL=0: do not use sequence parallel (default=1) [seq paraellel do not work for TP=1]

#bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=1 PP=1 BS=256 SEQ_LENGTH=2048 NO_TORCH_COMPILE=1 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=1 BS=256 SEQ_LENGTH=2048 NO_TORCH_COMPILE=1 CONTI_PARAMS=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=1 PP=2 BS=256 SEQ_LENGTH=2048 NO_TORCH_COMPILE=1 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 NO_TORCH_COMPILE=1 CONTI_PARAMS=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=4 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 NO_TORCH_COMPILE=1 CONTI_PARAMS=0

# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=1 PP=1 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=1 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=1 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=4 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0


# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=1 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=1 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=1
# bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=1

# bash train_llama2.sh MODEL_SIZE=13 MBS=4 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0
# bash train_llama2.sh MODEL_SIZE=13 MBS=4 TP=2 PP=2 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=1

#bash train_llama2.sh MODEL_SIZE=13 MBS=2 TP=1 PP=1 BS=256 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=0
bash train_llama2.sh MODEL_SIZE=70 MBS=1 TP=8 PP=1 BS=128 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=1
bash train_llama2.sh MODEL_SIZE=70 MBS=1 TP=8 PP=1 BS=128 SEQ_LENGTH=2048 CONTI_PARAMS=0 SEQ_PARALLEL=0 NO_TORCH_COMPILE=0

